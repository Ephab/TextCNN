{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP8d06DdcGosLqIscv0y5r3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget https://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "id": "K8qjN5ipb80V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2stF3D7BVtjM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import transforms\n",
        "from torch import nn\n",
        "import torchsummary\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "mXqcUeu5vsQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "training_set = dataset[\"train\"]\n",
        "testing_set = dataset[\"test\"]"
      ],
      "metadata": {
        "id": "JyJShm9xbvwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EDA"
      ],
      "metadata": {
        "id": "g5B4ZvtT2n3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.DataFrame(training_set)\n",
        "train_df = train_df.rename(columns={\"text\": \"training text\", \"label\": \"training label\"})\n",
        "\n",
        "test_df = pd.DataFrame(testing_set)\n",
        "test_df = test_df.rename(columns={\"text\": \"testing text\", \"label\": \"testing label\"})\n",
        "\n",
        "# 3. Concatenate\n",
        "df = pd.concat([train_df, test_df], axis=1)\n",
        "df.tail()"
      ],
      "metadata": {
        "id": "l10ixBGF2rHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(7, 7))\n",
        "\n",
        "train_counts = df[\"training label\"].value_counts()\n",
        "test_counts = df[\"testing label\"].value_counts()\n",
        "\n",
        "axes[0].pie(x=train_counts.values, labels=train_counts.index)\n",
        "axes[0].set_title(\"train label distribution\")\n",
        "\n",
        "axes[1].pie(x=test_counts.values, labels=test_counts.index)\n",
        "axes[1].set_title(\"test label distribution\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rS5uKF_v4S8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Emedding and model"
      ],
      "metadata": {
        "id": "bGSZa5n_2nuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GLOVE PRETRAINED EMBEDDINGS\n",
        "embedding_index = {}\n",
        "with open(\"glove.6B.100d.txt\") as file:\n",
        "    lines = file.readlines()\n",
        "    for line in lines:\n",
        "        line = line.split()\n",
        "        word = line[0]\n",
        "        embedding = np.array(line[1:], dtype=np.float32)\n",
        "        embedding_index[word] = embedding"
      ],
      "metadata": {
        "id": "9os6MjYOqwMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_index"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1tbzJJPp1Lpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CLEAN THE TRAINING DATA\n",
        "def keep_alphabet(text:str):\n",
        "    temp = \"\"\n",
        "    text = text.replace(\"<br />\", \" \")\n",
        "    for character in text:\n",
        "        if character.isalpha() or character == \" \":\n",
        "            temp += character\n",
        "    return temp"
      ],
      "metadata": {
        "id": "2SOWEqgFgQpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_lists_of_words(text:str):\n",
        "    return keep_alphabet(text).lower().split()"
      ],
      "metadata": {
        "id": "VzlyXWrxhs-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For preparing the dataset"
      ],
      "metadata": {
        "id": "9-OFyYGzSUZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frequency_of_words = {}\n",
        "def count_freq(words:list):\n",
        "    for word in words:\n",
        "        frequency_of_words[word] = frequency_of_words.get(word, 0) + 1"
      ],
      "metadata": {
        "id": "e0Fo5j0xiC7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for example in training_set[\"text\"]:\n",
        "    count_freq(prepare_lists_of_words(example))"
      ],
      "metadata": {
        "id": "fOxH9BaXdA2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frequencies_list = sorted(frequency_of_words.items(), key=lambda x: x[1], reverse=True) #frequency of each word in all our training examples\n",
        "\n",
        "word_to_index = {items[0]:index for index, items in enumerate(frequencies_list, start=2) if items[1] > 5} #start at 2 to leave space for padding and UNK\n",
        "word_to_index[\"<PAD>\"] = 0\n",
        "word_to_index[\"<UNK>\"] = 1\n",
        "\n",
        "list(word_to_index.items())[:5] #our word to index dict"
      ],
      "metadata": {
        "id": "dPYXexK4e0cG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up the embedding matrix"
      ],
      "metadata": {
        "id": "VirsjWKVScoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "UNK = np.random.rand(1,100)\n",
        "PAD = np.zeros((1,100),dtype=np.float32)\n",
        "\n",
        "embedding_matrix = np.zeros((len(word_to_index) + 2, 100)) #num words x 100 (embedding size)\n",
        "embedding_matrix[0] = PAD # set up the <PAD>\n",
        "embedding_matrix[1] = UNK # set up the <UNK>\n",
        "\n",
        "for word, index in word_to_index.items():\n",
        "    if word in embedding_index: # check if the word has an embedding in gloVe\n",
        "        embedding_matrix[index] = embedding_index[word]\n",
        "    else:\n",
        "        embedding_matrix[index] = UNK #else make it <UNK>"
      ],
      "metadata": {
        "id": "9XE1sdqPikXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix"
      ],
      "metadata": {
        "id": "fpj2pCyyprAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def words_to_id(listofwords:list):\n",
        "    ids = []\n",
        "    for word in listofwords:\n",
        "        if word in word_to_index:\n",
        "            ids.append(word_to_index[word])\n",
        "        else:\n",
        "            ids.append(1) #UNK\n",
        "    return ids\n",
        "words_to_id(['this', 'movie', 'curiousyellow'])"
      ],
      "metadata": {
        "id": "qcqaRubh64Ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_truncate(list_of_elements, max_length=100):\n",
        "    length = len(list_of_elements)\n",
        "    if length < max_length:\n",
        "        for i in range(abs(max_length-length)):\n",
        "            list_of_elements.append(word_to_index[\"<PAD>\"])\n",
        "\n",
        "    if length > max_length:\n",
        "        list_of_elements = list_of_elements[:max_length]\n",
        "\n",
        "    return list_of_elements"
      ],
      "metadata": {
        "id": "DLM2E7M89S93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "PROCESSING THE DATASET\n",
        "\n",
        "1- prepare_lists_of_words(text): Cleans raw text into a list of words.\n",
        "\n",
        "2- words_to_id(list_of_words): Converts words to their integer IDs.\n",
        "\n",
        "3- pad_truncate(list_of_ids): Forces the ID list to be max_length.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "7UHK1_Aw8a8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = []\n",
        "y_train = []\n",
        "X_test = []\n",
        "y_test = []\n",
        "\n",
        "for example in training_set:\n",
        "    text = example[\"text\"]\n",
        "    label = example[\"label\"]\n",
        "\n",
        "    words = prepare_lists_of_words(text)\n",
        "    ids = words_to_id(words)\n",
        "    ids = pad_truncate(ids)\n",
        "\n",
        "    X_train.append(ids)\n",
        "    y_train.append(label)\n",
        "\n",
        "for example in testing_set:\n",
        "    text = example[\"text\"]\n",
        "    label = example[\"label\"]\n",
        "\n",
        "    words = prepare_lists_of_words(text)\n",
        "    ids = words_to_id(words)\n",
        "    ids = pad_truncate(ids)\n",
        "\n",
        "    X_test.append(ids)\n",
        "    y_test.append(label)\n",
        "\n",
        "len(X_train[:1][0])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "OaYAARwmAUk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transform = transforms.ToTensor()\n",
        "X_train = torch.tensor(X_train, dtype=torch.long)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test = torch.tensor(X_test, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "torch_train_dataset = TensorDataset(X_train, y_train)\n",
        "torch_test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(torch_train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(torch_test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "YuolN3doBZvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextCNN(nn.Module):\n",
        "    def __init__(self, embedding_matrix):\n",
        "        super().__init__()\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        #empty embedding layer\n",
        "        self.embedding = nn.Embedding(num_embeddings=embedding_matrix.shape[0], embedding_dim=embedding_matrix.shape[1])\n",
        "\n",
        "        #embedding layer with trained weights\n",
        "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "\n",
        "        #freeze the params, we dont want to fine tune the embeddings for now.\n",
        "        self.embedding.weight.requires_grad = False\n",
        "\n",
        "        # 2-gram convs\n",
        "        self.gram2conv = nn.Conv1d(in_channels=100, out_channels=64, kernel_size=2)\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=99)\n",
        "\n",
        "        # 3-gram convs\n",
        "        self.gram3conv = nn.Conv1d(in_channels=100, out_channels=64, kernel_size=3) #(batch size, embedding dimension (in_channels), number of embeddings (sequence length)\n",
        "        self.pool3 = nn.MaxPool1d(kernel_size=98) #100 - 3 + 1 = 98\n",
        "\n",
        "        # 4-gram convs\n",
        "        self.gram4conv = nn.Conv1d(in_channels=100, out_channels=64, kernel_size=4)\n",
        "        self.pool4 = nn.MaxPool1d(kernel_size=97) #100 - 4 + 1 = 97\n",
        "\n",
        "        # 5-gram convs\n",
        "        self.gram5conv = nn.Conv1d(in_channels=100, out_channels=64, kernel_size=5)\n",
        "        self.pool5 = nn.MaxPool1d(kernel_size=96) # 100  - 5 + 1 = 96\n",
        "\n",
        "        #dropout\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "        #linear layer\n",
        "        self.linear = nn.Linear(in_features=256, out_features=2) # 2 output neurons\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # x will have shape of (batch size, number of words (ids) -> which then are converted into embeddings)\n",
        "        # so our x will be (batch size, number of embeddings (sequence / max length), embedding dimension)\n",
        "\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # Conv1d, however, takes inputs of (batch size, embedding dimension (in_channels), number of embeddings (sequence length))\n",
        "        # so we must somehow swap the max length and embedding dimension\n",
        "\n",
        "        x = x.permute(0, 2, 1) #put index 2 in index 1, and index 1 in index 2\n",
        "        # now x has shape of (batch size, embedding dimension, sequence length)\n",
        "\n",
        "        # 2-gram conv layers\n",
        "        x2 = self.gram2conv(x)\n",
        "        x2 = self.relu(x2)\n",
        "        x2 = self.pool2(x2)\n",
        "        x2 = torch.squeeze(x2, dim=2)\n",
        "\n",
        "        # 3-gram conv layers\n",
        "        x3 = self.gram3conv(x)\n",
        "        x3 = self.relu(x3)\n",
        "        x3 = self.pool3(x3) # now the shape of x is: (64, 32, 1) 64 batches, 32 rows of 1 column representing the max of each row\n",
        "        x3 = torch.squeeze(x3, dim=2) # make x (64, 32) instead of (64, 32, 1)\n",
        "\n",
        "        # 4-gram conv layers\n",
        "        x4 = self.gram4conv(x)\n",
        "        x4 = self.relu(x4)\n",
        "        x4 = self.pool4(x4)\n",
        "        x4 = torch.squeeze(x4, dim=2)\n",
        "\n",
        "        # 5-gram conv layers\n",
        "        x5 = self.gram5conv(x)\n",
        "        x5 = self.relu(x5)\n",
        "        x5 = self.pool5(x5)\n",
        "        x5 = torch.squeeze(x5, dim=2)\n",
        "\n",
        "        total_features = torch.hstack([x2, x3, x4, x5])\n",
        "\n",
        "        total_features = self.dropout(total_features)\n",
        "        total_features = self.linear(total_features)\n",
        "\n",
        "        return total_features\n"
      ],
      "metadata": {
        "id": "EXu8VJSRDdGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Loop"
      ],
      "metadata": {
        "id": "bpYMDo-svh6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = TextCNN(embedding_matrix=embedding_matrix).to(device)\n",
        "model = torch.compile(model)"
      ],
      "metadata": {
        "id": "Ril3M0k2vnNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torchsummary.summary(model, input_size=(100,))"
      ],
      "metadata": {
        "id": "a4rCbOgt82NH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.1)"
      ],
      "metadata": {
        "id": "onF3H6pBEvoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        for X, y in loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            output = model(X)\n",
        "            correct += (torch.argmax(output, dim=1) == y).sum().item()\n",
        "        model.train()\n",
        "        return correct / len(loader.dataset)\n"
      ],
      "metadata": {
        "id": "cPPmvo_Ly0WP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epochs):\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for X, y in train_loader:\n",
        "\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(X)\n",
        "\n",
        "            loss = loss_fn(output, y)\n",
        "\n",
        "            epoch_loss += loss.item() #keep running loss\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}   Training Loss= {epoch_loss/len(train_loader)} \", end=\"\")\n",
        "        print(f\"   Train Accuracy={get_accuracy(train_loader)}   Test Accuracy= {get_accuracy(test_loader)}\" if (epoch+1) % 5 == 0 else \"\")\n"
      ],
      "metadata": {
        "id": "ektzawHcw14y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(10)"
      ],
      "metadata": {
        "id": "6K0dg_ol0spd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iTB2gO7z_Twh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}